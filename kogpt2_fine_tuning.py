# -*- coding: utf-8 -*-
"""kogpt2_fine_tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ziGVR0Qhi_3J5xU80xUTd5Z3nrsJ5nAA
"""

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

data_a = pd.read_csv('/content/drive/MyDrive/gdsc/answer.csv')
data_q = pd.read_csv('/content/drive/MyDrive/gdsc/question.csv')

data_a

data_q

data = pd.read_csv('/content/drive/MyDrive/gdsc/chat_data.csv')

data

chat_data = data[['question', 'answer']]

chat_data

chat_data.to_csv('chat_train.csv', index = False)

test = chat_data[:300]
test

test.to_csv('chat_test.csv', index = False)

!pip install pytorch-lightning

!pip install transformers

tra = pd.read_csv('/content/drive/MyDrive/gdsc/chat_train.csv')
tra

te = pd.read_csv('/content/drive/MyDrive/gdsc/chat_test.csv')

te

import math
import pandas as pd
import numpy as np
import random
import re
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import PreTrainedTokenizerFast

train = pd.read_csv('/content/drive/MyDrive/gdsc/chat_train.csv')
test = pd.read_csv('/content/drive/MyDrive/gdsc/chat_test.csv')

BOS = "</s>"
EOS = "</s>"
PAD = "<pad>"
MASK = "<unused0>"
tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2", bos_token = BOS, eos_token = EOS, unk_token = "<unk>", pad_token = PAD, mask_token = MASK)

Q_TKN = "<usr>"
A_TKN = "<sys>"
BOS = "</s>"
EOS = "</s>"
PAD = "<pad>"
MASK = "<unused0>"
SENT = "<unused1>"

import math
import pandas as pd
import numpy as np
import random
import re
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import PreTrainedTokenizerFast

train = pd.read_csv('/content/drive/MyDrive/gdsc/chat_train.csv')
test = pd.read_csv('/content/drive/MyDrive/gdsc/chat_test.csv')

Q_TKN = "<usr>"
A_TKN = "<sys>"
BOS = "</s>"
EOS = "</s>"
PAD = "<pad>"
MASK = "<unused0>"
SENT = "<unused1>"
tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2", bos_token = BOS, eos_token = EOS, unk_token = "<unk>", pad_token = PAD, mask_token = MASK)

class fundchatDataset(Dataset):
    def __init__(self, chats, max_len = 100):
        self._data = chats
        self.max_len = max_len
        self.q_token = Q_TKN
        self.a_token = A_TKN
        self.sent_token = SENT
        self.eos = EOS
        self.mask = MASK
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self._data)

    def __getitem__(self, idx):
        turn = self._data.iloc[idx]
        q = turn["question"]
        q = re.sub(r"([?.!,])", r" ", q)

        a = turn["answer"]
        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)
        q_len = len(q_toked)

        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)
        a_len = len(a_toked)

        if q_len > self.max_len:
            a_len = self.max_len - q_len
            if a_len <= 0:
                q_toked = q_toked[-(int(self.max_len / 2)) :]
                q_len = len(q_toked)
                a_len = self.max_len - q_len
            a_toked = a_toked[:a_len]
            a_len = len(a_toked)

        if q_len + a_len > self.max_len:
            a_len = self.max_len - q_len
            if a_len <= 0:
                q_toked = q_toked[-(int(self.max_len / 2)) :]
                q_len = len(q_toked)
                a_len = self.max_len - q_len
            a_toked = a_toked[:a_len]
            a_len = len(a_toked)

        labels = [self.mask,] * q_len + a_toked[1:]

        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)
        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)
        while len(labels_ids) < self.max_len:
            labels_ids += [self.tokenizer.pad_token_id]

        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)
        while len(token_ids) < self.max_len:
            token_ids += [self.tokenizer.pad_token_id]
        return (token_ids, np.array(mask), labels_ids)

    def collate_batch(batch):
        data = [item[0] for item in batch]
        mask = [item[1] for item in batch]
        return torch.LongTensor(data), torch.LongTensor(mask)

import argparse
import numpy as np
import pandas as pd
import torch
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.core import LightningModule
from torch.utils.data import DataLoader, Dataset
from transformers.optimization import AdamW, get_cosine_schedule_with_warmup
from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel
import re

parser = argparse.ArgumentParser()
parser.add_argument("--epochs", default = 10, type = int)
parser.add_argument("--lr", default = 0.002, type = float)
parser.add_argument("--batch_size", default = 32, type = int)
parser.add_argument("--warmup_steps", default = 200, type = int)
args = parser.parse_args('')

def collate_batch(batch):
    data = [item[0] for item in batch]
    mask = [item[1] for item in batch]
    return torch.LongTensor(data), torch.LongTensor(mask)

tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2", bos_token = BOS, eos_token = EOS, unk_token = "<unk>", pad_token = PAD, mask_token = MASK)
model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')

train_data = pd.read_csv('/content/drive/MyDrive/gdsc/chat_train.csv')
test_data = pd.read_csv('/content/drive/MyDrive/gdsc/chat_test.csv')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
train_set = fundchatDataset(test_data, max_len = 100)
train_dataloader = DataLoader(train_set, batch_size = args.batch_size, num_workers = 0, shuffle = True, collate_fn = collate_batch,)

model.to(device)
model.train()

criterion = torch.nn.CrossEntropyLoss(reduction = "none")
optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)
Sneg = -1e18

print("start")
for epoch in range(args.epochs):
    for batch_idx, samples in enumerate(train_dataloader):
        optimizer.zero_grad()
        token_ids, mask = samples
        out = model(token_ids)
        out = out.logits
        mask_3d = mask.unsqueeze(dim = 2).repeat_interleave(repeats = out.shape[2], dim = 2)
        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))
        loss = criterion(mask_out.transpose(2, 1), answer)
        avg_loss = loss.sum() / mask.sum()
        avg_loss.backward()
        optimizer.step()
print("end")

